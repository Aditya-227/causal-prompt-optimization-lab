# -*- coding: utf-8 -*-
"""Causal Prompt Optimization Lab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wgYgvHK1NnpL3OAtPACwsbeYNW1tCtw-
"""

!pip install -q transformers accelerate bitsandbytes datasets pandas numpy statsmodels scikit-learn matplotlib seaborn tqdm streamlit

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

model.eval()

from datasets import load_dataset

dataset = load_dataset("gsm8k", "main")
questions = dataset["test"].select(range(12))

def extract_answer(ans):
    return ans.split("####")[-1].strip()

data = []
for i, item in enumerate(questions):
    data.append({
        "question_id": i,
        "question": item["question"],
        "answer": extract_answer(item["answer"])
    })

import itertools
import pandas as pd

factors = ["role", "cot", "fewshot", "constraint"]

# Generate full design
design = list(itertools.product([0,1], repeat=4))
df_design = pd.DataFrame(design, columns=factors)

# Balanced reduction
df_design = df_design.sample(8, random_state=42).reset_index(drop=True)
df_design

FEWSHOT_EXAMPLE = """
Q: If 2+2=?
A: 4
"""

def build_prompt(q, role, cot, fewshot, constraint):
    prompt = ""

    if role:
        prompt += "You are a careful mathematical reasoning assistant.\n"

    if fewshot:
        prompt += FEWSHOT_EXAMPLE + "\n"

    prompt += f"Question: {q}\n"

    if cot:
        prompt += "Think step by step.\n"

    if constraint:
        prompt += "Return only the final numeric answer.\n"

    return prompt

import time
from tqdm import tqdm

results = []

for item in data:
    for _, cond in df_design.iterrows():

        prompt = build_prompt(
            item["question"],
            cond.role,
            cond.cot,
            cond.fewshot,
            cond.constraint
        )

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        input_tokens = inputs.input_ids.shape[1]

        start = time.time()

        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0
        )

        latency = time.time() - start

        output_tokens = outputs.shape[1] - input_tokens
        total_tokens = outputs.shape[1]

        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        pred = decoded.split("\n")[-1].strip()

        correct = int(item["answer"] in pred)

        results.append({
            "question_id": item["question_id"],
            "role": cond.role,
            "cot": cond.cot,
            "fewshot": cond.fewshot,
            "constraint": cond.constraint,
            "correct": correct,
            "latency": latency,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens
        })

df = pd.DataFrame(results)
df.to_csv("causal_prompt_results.csv", index=False)

import statsmodels.formula.api as smf

model_acc = smf.ols(
    "correct ~ role + cot + fewshot + constraint + C(question_id)",
    data=df
).fit()

print(model_acc.summary())

import numpy as np

boot_coefs = []

for _ in range(1000):
    sampled_ids = np.random.choice(df.question_id.unique(),
                                    size=12,
                                    replace=True)
    boot_df = pd.concat([
        df[df.question_id == i] for i in sampled_ids
    ])

    m = smf.ols(
        "correct ~ role + cot + fewshot + constraint + C(question_id)",
        data=boot_df
    ).fit()

    boot_coefs.append(m.params)

boot_df = pd.DataFrame(boot_coefs)
ci = boot_df.quantile([0.025, 0.975])
ci

model_tokens = smf.ols(
    "total_tokens ~ role + cot + fewshot + constraint + C(question_id)",
    data=df
).fit()
print(model_tokens.summary())

model_latency = smf.ols(
    "latency ~ role + cot + fewshot + constraint + C(question_id)",
    data=df
).fit()
print(model_latency.summary())

import matplotlib.pyplot as plt

effects = model_acc.params[["role","cot","fewshot","constraint"]]

plt.bar(effects.index, effects.values)
plt.title("Main Effects on Accuracy")
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Main coefficients
coef = model_acc.params[["role","cot","fewshot","constraint"]]

# Bootstrap CI (from your ci dataframe)
lower = ci.loc[0.025, ["role","cot","fewshot","constraint"]]
upper = ci.loc[0.975, ["role","cot","fewshot","constraint"]]

errors = [coef - lower, upper - coef]

plt.figure()
plt.errorbar(coef.index, coef.values,
             yerr=errors,
             fmt='o',
             capsize=5)

plt.axhline(0)
plt.title("Main Effects with 95% Cluster Bootstrap CI")
plt.ylabel("Effect on Accuracy")
plt.show()

import matplotlib.pyplot as plt

agg = df.groupby(["role","cot","fewshot","constraint"]).agg({
    "correct":"mean",
    "total_tokens":"mean"
}).reset_index()

plt.figure()
plt.scatter(agg["total_tokens"], agg["correct"])

plt.xlabel("Average Total Tokens")
plt.ylabel("Accuracy")
plt.title("Accuracy vs Token Cost Tradeoff")

plt.show()

model_inter = smf.ols(
    "correct ~ role*cot + role*fewshot + cot*fewshot + constraint + C(question_id)",
    data=df
).fit()

print(model_inter.summary())

import pandas as pd
import matplotlib.pyplot as plt

interaction = df.groupby(["cot","fewshot"])["correct"].mean().unstack()

plt.figure()
plt.plot(interaction.index, interaction[0])
plt.plot(interaction.index, interaction[1])
plt.xlabel("Chain-of-Thought")
plt.ylabel("Accuracy")
plt.title("CoT × Few-shot Interaction")
plt.show()

model_inter_cluster = smf.ols(
    "correct ~ cot*fewshot + role + constraint + C(question_id)",
    data=df
).fit(
    cov_type="cluster",
    cov_kwds={"groups": df["question_id"]}
)

print(model_inter_cluster.summary())

import numpy as np
import matplotlib.pyplot as plt

means = df.groupby(["cot","fewshot"])["correct"].mean().reset_index()

x = means["cot"]
y = means["correct"]

plt.figure()
plt.scatter(x + means["fewshot"]*0.05, y)
plt.xlabel("Chain-of-Thought")
plt.ylabel("Mean Accuracy")
plt.title("Marginal Means: CoT and Few-shot")
plt.show()

model_tokens_cluster = smf.ols(
    "total_tokens ~ cot + fewshot + role + constraint + C(question_id)",
    data=df
).fit(
    cov_type="cluster",
    cov_kwds={"groups": df["question_id"]}
)

print(model_tokens_cluster.summary())

model_latency_cluster = smf.ols(
    "latency ~ cot + fewshot + role + constraint + C(question_id)",
    data=df
).fit(
    cov_type="cluster",
    cov_kwds={"groups": df["question_id"]}
)

print(model_latency_cluster.summary())

df["accuracy_per_token"] = df["correct"] / df["total_tokens"]
df["accuracy_per_second"] = df["correct"] / df["latency"]

eff = df.groupby(["cot","fewshot","role","constraint"]).agg({
    "correct":"mean",
    "total_tokens":"mean",
    "latency":"mean",
    "accuracy_per_token":"mean",
    "accuracy_per_second":"mean"
}).reset_index()

eff.sort_values("accuracy_per_token", ascending=False).head()

eff.sort_values("correct", ascending=False).head()

import matplotlib.pyplot as plt

plt.figure()
plt.scatter(eff["total_tokens"], eff["correct"])

for i, row in eff.iterrows():
    label = f"C{row.cot}F{row.fewshot}R{row.role}K{row.constraint}"
    plt.annotate(label, (row["total_tokens"], row["correct"]), fontsize=8)

plt.xlabel("Mean Tokens")
plt.ylabel("Mean Accuracy")
plt.title("Token–Accuracy Frontier")
plt.show()

from google.colab import files
files.download("causal_prompt_results.csv")

!pip install nbformat

import nbformat

with open("Causal_Prompt_Optimization_Lab.ipynb") as f:
    nb = nbformat.read(f, as_version=4)

with open("clean_notebook.ipynb", "w") as f:
    nbformat.write(nb, f)

